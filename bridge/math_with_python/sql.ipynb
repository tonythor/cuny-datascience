{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67ac076-ce6c-4380-9a59-374a989cfe78",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBootstrap\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import logging\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "class Bootstrap:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_config(self, ini_fn: str = f\"resources/{os.getenv('MODULE')}.ini\") -> configparser.ConfigParser:\n",
    "        ini_path = self.get_resource(ini_fn)\n",
    "        config = configparser.ConfigParser()\n",
    "        logging.info(f\"loading config file from:{str(ini_path)}\")\n",
    "        config.read([str(ini_path)])\n",
    "        return config\n",
    "\n",
    "    def running_on_ecs(self) -> bool:\n",
    "        if  os.environ.get(\"AWS_CONTAINER_CREDENTIALS_RELATIVE_URI\"):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_spark(self):\n",
    "        import botocore.session\n",
    "        from pyspark.sql import SparkSession\n",
    "        from pyspark import SparkConf\n",
    "\n",
    "        session = botocore.session.get_session()\n",
    "        credentials = session.get_credentials()\n",
    "        conf = SparkConf()\n",
    "        conf.set('spark.hadoop.fs.s3a.access.key', credentials.access_key)\n",
    "        conf.set('spark.hadoop.fs.s3a.secret.key', credentials.secret_key)\n",
    "        conf.set(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        conf.set('spark.driver.memory', '4g')\n",
    "        conf.set('spark.executor.memory', '4g')\n",
    "\n",
    "        if self.running_on_ecs():\n",
    "            #overwrite provider to use session token, doesn't seem to work as a array, event though it is supposed to.\n",
    "            conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "            conf.set(\"spark.hadoop.fs.s3a.session.token\", credentials.token)\n",
    "        spark = (\n",
    "            SparkSession\n",
    "                .builder\n",
    "                .config(conf=conf)\n",
    "                .appName(f\"my-executor\")\n",
    "                .getOrCreate()\n",
    "        )\n",
    "        self.spark = spark\n",
    "        self.spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "        return self.spark\n",
    "\n",
    "    def kill_spark(self):\n",
    "        \"\"\"\n",
    "        <i>If necessary, used to close spark after job completion.</i>\n",
    "        \"\"\"\n",
    "        self.spark.stop()\n",
    "\n",
    "    def set_log_level(self, level=\"warning\"):\n",
    "        \"\"\"\n",
    "        A shortcut to help ipython or jupyter users using bootstrap to adjust logging between code blocks.\n",
    "        Use this to make sure you're updating all the loggers.\n",
    "        ::\n",
    "            bootstrap.set_log_level('debug')\n",
    "        \"\"\"\n",
    "        if level == \"warning\":\n",
    "            logging.getLogger().setLevel(logging.WARNING)\n",
    "        if level == \"error\":\n",
    "            logging.getLogger().setLevel(logging.ERROR)\n",
    "        if level == \"info\":\n",
    "            logging.getLogger().setLevel(logging.INFO)\n",
    "        if level == \"debug\":\n",
    "            logging.getLogger().setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce2e8c5-105c-4dd1-a15d-42f8fb8e7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Bootstrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5420c479-a63e-4955-a18d-06c6632f1e7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mBootstrap.get_spark\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_spark\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf\n\u001b[1;32m     27\u001b[0m     session \u001b[38;5;241m=\u001b[39m botocore\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget_session()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "spark = b.get_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e10428f-394e-444a-b324-07a859d68834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
